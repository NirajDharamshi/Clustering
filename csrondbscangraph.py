# -*- coding: utf-8 -*-
"""csrondbscangraph.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h_KSy1gj7NUmcWMlEI8fknOtMbE64yXH
"""

import numpy as np
import pandas as pd
import random
from collections import defaultdict
from scipy.sparse import csr_matrix
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import Normalizer
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt
import numpy

#reading csr file and returning csr matrix by calculating ind,ptr and values in the file.
def csr_read(fname, ftype="csr", nidx=1):
    
    with open(fname) as f:
        lines = f.readlines()
        nrows = len(lines)
        ncols = 0 
        nnz = 0 
        for i in range(nrows):
            p = lines[i].split()
            if len(p) % 2 != 0:
                raise ValueError("Invalid CSR matrix")
            nnz += len(p)/2
            nnz=int(nnz)
            for j in range(0, len(p), 2): 
                cid = int(p[j]) - nidx
                if cid+1 > ncols:
                    ncols = cid+1
    
    val = np.zeros(nnz, dtype=np.float)
    ind = np.zeros(nnz, dtype=np.int)
    ptr = np.zeros(nrows+1, dtype=np.long)
    n = 0 
    for i in range(nrows):
        p = lines[i].split()
        for j in range(0, len(p), 2): 
            ind[n] = int(p[j]) - nidx
            val[n] = float(p[j+1])
            n += 1
        ptr[i+1] = n 
    
    assert(n == nnz)
    
    return csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.float)

#normalising csr matrix or term frequency by inverse document frequency to minimise impact of frequent words
#returns normalised matrix
def csr_idf(matrix, copy=False, **kargs):

    if copy is True:
        matrix = matrix.copy()
    nrows = matrix.shape[0]
    nnz = matrix.nnz
    ind, val, ptr = matrix.indices, matrix.data, matrix.indptr
    # document frequency
    df = defaultdict(int)
    for i in ind:
        df[i] += 1
    # inverse document frequency
    for k,v in df.items():
        df[k] = np.log(nrows / float(v)) 
    # scale by idf
    for i in range(0, nnz):
        val[i] *= df[ind[i]]
        
    return df if copy is False else matrix

#read csr & normalise csr
csr = csr_read('train.dat', ftype="csr", nidx=1)
csrMatrix=csr_idf(csr, copy=True)

#performing dimensionality reduction using SVD to 150 components and normalising the output to inhibit impact of document length on distance computation
    print("Performing dimensionality reduction using LSA")
    svd = TruncatedSVD(n_components=150, random_state=42)
    normalizer = Normalizer(copy=False)
    X=svd.fit_transform(csrMatrix)
    points=normalizer.fit_transform(X)

    explained_variance = svd.explained_variance_ratio_.sum()
    print("Explained variance of the SVD step: {}%".format(
        int(explained_variance * 100)))

#Computing eps for minpts using K-distance graph and elbow point in it.
nbrs = NearestNeighbors(n_neighbors=21,metric='euclidean').fit(points)
distances, indices = nbrs.kneighbors(points)
t=distances[:,-1]
i=indices[:,0]
a=np.sort(t)
plt.axis([0, 8680, 0, 1.2])
plt.plot(i,a)

#setting minpts & eps(radius) for DBSCAN algorithms input
minpts=4
eps=0.1102

#computing core points
    core=[]
    for i in range(len(points)):
      neighbors = []
     
      for Pn in range(0, len(points)):
        
        # If the distance is below the threshold, add it to the neighbors list.
        if numpy.linalg.norm(points[i] - points[Pn]) < eps:
          neighbors.append(Pn)
        #if neighbors are more than minpts than add it to core list
      if len(neighbors) > minpts :
          core.append(i)

#computing border points
    border=[]
    for i in range(len(points)):
      neighbors = []
   
      for Pn in range(0, len(points)):
        
        # If the distance is below the threshold, add it to the neighbors list.
        if numpy.linalg.norm(points[i] - points[Pn]) < eps:
          neighbors.append(Pn)
          # check if neighbors contain any core,if yes than add it to border point list
      if len(neighbors) < minpts:
        for j in range(len(neighbors)):
            if neighbors[j] in core:
              border.append(i)
              break

#Points which are neither core nor border are noise points
noise=[]

for i in range(len(points)):
  if i not in core and i not in border:
    noise.append(i)

#checking total of all the points
len(noise)+len(core)+len(border)

#with original total points
len(points)

#creating graph to store core points in neighbourhood
g = {item: [-1] for item in core}

for i in range(len(core)):
      
   
      for Pn in range(0, len(core)):
        
        # If the distance is below the threshold than core points are connected in graph
        if numpy.linalg.norm(points[core[i]] - points[core[Pn]]) < eps:
          g[core[i]].append(core[Pn])

#removing default value added earlier
  for i,j in g.items():
    g[i].remove(-1)

#creating function to find connected core points
def get_all_connected_groups(graph):
    already_seen = set()
    result = []
    for node in graph:
        if node not in already_seen:
            connected_group, already_seen = get_connected_group(node, already_seen,graph)
            result.append(connected_group)
    return result

def get_connected_group(node, already_seen,graph):
        result = []
        nodes = set([node])
        while nodes:
            node = nodes.pop()
            already_seen.add(node)
            nodes = nodes or set(graph[node]) - already_seen
            result.append(node)
        return result, already_seen

#all the connected graphs are saved in list
components = get_all_connected_groups(g)

#Border points are added to nearest core point connected graph list
    count=0
    for i in range(len(border)):
      neighbors = []
     # print('--------------------------')
      for Pn in range(len(components)):
        for j in range(len(components[Pn])):
          
        # If the distance is below the threshold, add it to the neighbors list.
          if numpy.linalg.norm(points[components[Pn][j]] - points[border[i]]) < eps:
             count=count+1
        neighbors.append(count)
        count=0
      cluster_num=neighbors.index(max(neighbors))
      components[cluster_num].append(border[i])

labels={}

#Alloting labels to all elements in connected graph
for i in range(1,len(components)+1):
  for j in range(len(components[i-1])):
    labels[components[i-1][j]]=i
  no=i

#create separate cluster for noise points
for i in range(len(noise)):
  labels[noise[i]]=no+1

#creating list as per sequence of the original points
dictlist=[]
for i in range(len(labels)):
    dictlist.append(labels[i])

set(dictlist)

#results are written to .dat file
with open('e903520.dat', 'w') as f:
    for item in dictlist:
        f.write("%s\n" % item)

from sklearn import metrics

s=metrics.calinski_harabaz_score(points, dictlist)

#precomputed as it takes lot of memory and time to compute score
score=[79.879,75.362,74.8451,72.3187,66.812,15.25,26.31,24.63,9.28,6.44]

k=[3,5,7,9,11,13,15,17,19,21]
plt.xlabel('Minpts')
plt.ylabel('Calinski and Harabaz Score')
plt.plot(k,score)